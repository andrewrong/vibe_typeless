"""
AI æ–‡æœ¬åå¤„ç†æµ‹è¯•è„šæœ¬
æ”¯æŒ OpenAI, Gemini, Ollama
é…ç½®é€šè¿‡ .env æ–‡ä»¶ç®¡ç†
"""
import sys
import asyncio

sys.path.insert(0, '.')

from src.postprocess.ai_processor import AIPostProcessor, PostProcessRequest
from src.config import settings

# æµ‹è¯•ç”¨ä¾‹
test_cases = [
    {
        "name": "åŸºæœ¬ä¼˜åŒ– - ç§»é™¤å¡«å……è¯",
        "input": "å—¯ é‚£ä¸ª äº”ä¸ª äº‹æƒ… é¦–å…ˆ æˆ‘ä»¬éœ€è¦ åš API æ¥å£ è®¾è®¡ ç„¶å å®ç° å®ƒ æœ€å æµ‹è¯• å®ƒ",
        "expected_keywords": ["5ä»¶äº‹", "API", "1.", "2.", "3.", "4.", "5."],
    },
    {
        "name": "æ•°å­—è½¬æ¢",
        "input": "æˆ‘ä¹°äº† ä¸‰ä¸ª iPhone å’Œ äºŒå ç¾å…ƒçš„é…ä»¶",
        "expected_keywords": ["3", "iPhone", "$20"],
    },
    {
        "name": "åˆ—è¡¨æ ¼å¼åŒ–",
        "input": "è¦åšä¸‰ä»¶äº‹ ç¬¬ä¸€ ç¼–å†™ä»£ç  ç¬¬äºŒ æµ‹è¯• ç¬¬ä¸‰ éƒ¨ç½²",
        "expected_keywords": ["3ä»¶äº‹", "1.", "2.", "3."],
    },
    {
        "name": "ä¸­è‹±æ–‡æ··åˆ",
        "input": "æˆ‘ä»¬åœ¨ GitHub ä¸Šæ‰¾åˆ° Docker é•œåƒ ç„¶å ä¸‹è½½ å®ƒ",
        "expected_keywords": ["GitHub", "Docker", "ä¸‹è½½"],
    },
    {
        "name": "æ®µè½ç»„ç»‡",
        "input": "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬åŒ…å«å¾ˆå¤šå†…å®¹éœ€è¦åˆ†æˆå¤šä¸ªæ®µè½æ¥æé«˜å¯è¯»æ€§åº”è¯¥æœ‰æ›´å¥½çš„ç»“æ„",
        "expected_keywords": ["æ®µè½"],
    },
    {
        "name": "å»é‡å¤",
        "input": "ä½ å¥½ ä½ å¥½ æˆ‘æƒ³ æˆ‘æƒ³ è¯´ å»å•†åº—",
        "expected_keywords": ["ä½ å¥½", "æƒ³", "è¯´", "å»å•†åº—"],
    },
]


async def test_provider(processor: AIPostProcessor, provider: str, model: str):
    """æµ‹è¯•æŒ‡å®šæä¾›å•†"""
    print(f"\n{'=' * 60}")
    print(f"æµ‹è¯•æä¾›å•†: {provider} ({model})")
    print('=' * 60)

    # è¿è¡Œæµ‹è¯•
    for i, test in enumerate(test_cases, 1):
        print(f"\n{'=' * 60}")
        print(f"æµ‹è¯• {i}/{len(test_cases)}: {test['name']}")
        print('=' * 60)

        print(f"\nè¾“å…¥ ({len(test['input'])} å­—ç¬¦):")
        print(f"  {test['input']}")

        try:
            request = PostProcessRequest(
                text=test['input'],
                provider=provider,
                model=model
            )

            response = await processor.process(request)

            print(f"\nè¾“å‡º ({len(response.processed)} å­—ç¬¦):")
            print(f"  {response.processed}")

            # æ£€æŸ¥æœŸæœ›å…³é”®è¯
            missing = []
            for keyword in test['expected_keywords']:
                if keyword not in response.processed:
                    missing.append(keyword)

            if missing:
                print(f"\nâš ï¸  æœªæ‰¾åˆ°å…³é”®è¯: {', '.join(missing)}")
            else:
                print(f"\nâœ… æ‰€æœ‰å…³é”®è¯éƒ½æ‰¾åˆ°äº†")

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


async def test_ai_processor():
    """æµ‹è¯• AI å¤„ç†å™¨"""
    print("=" * 60)
    print("AI æ–‡æœ¬åå¤„ç†æµ‹è¯•")
    print("=" * 60)

    # æ˜¾ç¤ºå½“å‰é…ç½®
    print(f"\nğŸ“‹ å½“å‰é…ç½®ï¼ˆä» .env è¯»å–ï¼‰:")
    print(f"   AI_PROVIDER: {settings.AI_PROVIDER}")
    print(f"   ENABLE_AI_POSTPROCESS: {settings.ENABLE_AI_POSTPROCESS}")

    # æ”¶é›†å¯ç”¨çš„æä¾›å•†
    available_providers = []

    # æ£€æŸ¥ OpenAI
    if settings.OPENAI_API_KEY:
        available_providers.append(("openai", settings.OPENAI_MODEL))
        if settings.OPENAI_BASE_URL:
            print(f"\nâœ… OpenAI å·²é…ç½®ï¼ˆä½¿ç”¨ä»£ç†å•†: {settings.OPENAI_BASE_URL}ï¼‰")
        else:
            print(f"\nâœ… OpenAI å·²é…ç½®")
    else:
        print(f"\nâš ï¸  æœªè®¾ç½® OPENAI_API_KEYï¼Œè·³è¿‡ OpenAI æµ‹è¯•")

    # æ£€æŸ¥ Gemini
    if settings.GEMINI_API_KEY:
        available_providers.append(("gemini", settings.GEMINI_MODEL))
        print(f"âœ… Gemini å·²é…ç½®")
    else:
        print(f"âš ï¸  æœªè®¾ç½® GEMINI_API_KEYï¼Œè·³è¿‡ Gemini æµ‹è¯•")

    # æ£€æŸ¥ Ollama
    import requests
    try:
        response = requests.get(settings.OLLAMA_BASE_URL + "/api/tags", timeout=2)
        available_providers.append(("ollama", settings.OLLAMA_MODEL))
        print(f"âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸ @ {settings.OLLAMA_BASE_URL}")
    except Exception:
        print(f"âš ï¸  Ollama æœåŠ¡æœªè¿è¡Œ @ {settings.OLLAMA_BASE_URL}ï¼Œè·³è¿‡ Ollama æµ‹è¯•")

    if not available_providers:
        print("\nâŒ é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„ AI æä¾›å•†")
        print("\nè¯·åœ¨ PythonService/.env æ–‡ä»¶ä¸­é…ç½®è‡³å°‘ä¸€ä¸ªæä¾›å•†:")
        print("  OPENAI_API_KEY=sk-xxx")
        print("  GEMINI_API_KEY=xxx")
        print("  æˆ–å¯åŠ¨ Ollama: ollama serve &")
        return

    # åˆ›å»ºå¤„ç†å™¨
    processor = AIPostProcessor()

    # æµ‹è¯•æ¯ä¸ªå¯ç”¨çš„æä¾›å•†
    for provider, model in available_providers:
        await test_provider(processor, provider, model)

    print(f"\n{'=' * 60}")
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(test_ai_processor())
